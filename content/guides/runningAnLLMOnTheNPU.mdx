---
title: ðŸš§(WIP)ðŸš§ Running an LLM on the NPU
---
>[!CAUTION]
>
>ðŸš§Work in ProgressðŸš§<br />
>You could run in to some Problems.<br />
Be aware of what you do and its consequences.
# ðŸš§(Work in Progress)ðŸš§ Running an .rkllm model on the NPU
Needed Packages:
```bash copy
pkcon install git
pkcon install pip
pkcon install libgomp
```

Gitrepo with an Ollama compatible server:
```basch copy
git clone https://github.com/notpunchnox/rkllama
```
```bash copy
cd rkllama
```
```bash copy
pip install -r requirements.txt
```
>```bahs copy
>pip intall transformers
>pip install flask
>pip install huggingface_hub
>pip install dotenv
>pip install requests
>```
```bash copy
chmod +x setup.sh
./setup.sh --no-conda
```

set CPU to rk3588 in serve.sh
```bash filename="serve.sh"
8 CPU_MODEL="" #Change to CPU_MODEL="rk3588"
9 PORT=8080 #Change to something that's not 8080 like 8084
```

replace ```{"--no-conda" if use_no_conda else ""}``` with ```--no-conda``` in serve.py
```python filename="client.py"
357 os.system(f"bash ~/RKLLAMA/server.sh {"--no-conda" if use_no_conda else ""} --port={PORT}")
```
```python filename="client.py"
357 os.system(f"bash ~/RKLLAMA/server.sh {"--no-conda" if use_no_conda else ""} --port={PORT}")
```

## Run a Model from a self converted Model file or pull it from Hugging Face
```
#Switch to where it is installed
cd /root/rkllama
chmod +x client.sh
./client.sh --no-conda
```
self converted Model
Just add it to the /root/RKLLAMA/models folder

Hugging Face download a model
```bash copy
./client.sh --no-conda serve
#open new terminal or coninue running it in the Background
./client --no-conda pull
#Follow the steps in the Terminal there and enter the Repo Path then the model filename
```

```bash copy
./client run 'MODELNAME'
```

## Soruce
https://github.com/NotPunchnox/rkllama/